Design patterns
---------------

3.x

Basic
-----


LB/API            LB/APPS  

3-Masters         3-Infra      3-App/compute  1/2-Bastion 



Advance
-------
LB/API                 LB/APPS  

3-Masters/etcd         2-Infra      2-App/compute  2-Logging-Nodes   2-Monitoring-Nodes  1 Bastion




* Jenkins CI/CD on infra or application/Compute ? 
* https://access.redhat.com/solutions/2806991


Wildcard DNS : *.cloudapps.example.com ----> InfraLB
Or 
Bind/Name/DNS Server under your control


4.X
---


LBs
API                 
APPS  

Cluster Term
-------------

BootStrap Machine 
CP
Workers
Based on Labeling Routing,Monitoring & Alerting (Exempted from Subscriptions) 


1 Bootstrap 
3-ControlPlane colocated ETCD         
2-Routing        
2-Logging  
2-Monitoring & alerting 





Deployment 
----------

1. Internet access to pull images for platform containers 
2. Provide telemetry data to Red Hat
3. Openshift Infra provider page : 
4. https://cloud.redhat.com/openshift/install
5. https://access.redhat.com/articles/4128421


Baremetal 
---------

RHCOS
1. ISO image 
2. Network PXE booting 
3. DNS 
4. DHCP 
5. HTTP server


Network-Segment
---------------
---------------

L2 - MacAddress
L3 - Ip Address 


Underlay 
- Physical network layer 
- Protocol VLAN & Routing (BGP, OSPF, IS-IS) 
- 4096 Vlan segments


Overlay ----is our SDN story 

- Virtual NL over Underlay 
- Protocol VXLAN
- 16 Million
- Encapsulation Overhead 

OCP-on-OSP 

Common networking model 
-----------------------

Who provision ?
Provider network - OSP admin - underlay
Tenant network - Tenant - Overlay 
* can be vice versa 


Service Discovery 
-----------------
Q. How services , pods , EPs , applications discover each other within cluster ?
A. In-built DNS (Sky-to-Core)

- Pods IPs and HN are ephermal 
- Services provides application tenacity 

Kube-proxy : Runs on each node, Stream forwarding to end points TCP/UDP/SCTP Round robin 



---
etcd ---- Nodes network Info registers
----

Over-lay-network created using OVS
----------------------------------

Enable Communication 

- Pod to Pod 
- Service to Service 
- Nodes to services/pods 
* without NAT 
- Maintain register
- Assign/Unassign available cluster network subnet to node


TYPES OF SDN
------------

1. ovs-subnet Flat no isolations for P-to-P & S-to-S

2. ovs-multitenant Project level isolations for P-to-P & S-to-S
"Virtual Network ID (VNID) that identifies traffic from pods assigned to the project"
"0" is priv

3. ovs-networkpolicy (Flat by default, Project administration Granular network isolation using NetworkPolicy objects)
NP: object name "networkpolicy"
- Denyall
- Allow router only
- Only from POD
- Pod with specific Label
- Protocol , HTTP/S
- Ports 
* Addictive ++++++
* Public cloud best practise ruled out ovs-subnet 
* Networkpolicy default mode in 4.x 
* Ovssubnet default mode in 3.x 


Packet-Flow
-----------

Pods on same Node - Linux br0
Pods on Remote Node - vxlan_sys_4789 port 1 
External network access :tun0 Port 2 



Subnets
-------
Cluster Network or POD network - 10.128.0.0/14 hosts 262144 (Multiple CIDR)
Service Network - 172.30.0.0/16 host 65536 (Single CIDR) 
Nodes Allocation - 10.128.0.0/23 
1. 2. 3. 4.....
510 510 510 510 ... Ips--to--pods/containers 


CNI:
Directory: 
/etc/cni/net.d
/opt/cni/bin
network interface into the container
assign the IP to the interface
setup the routes
IPAM plugin

Kubernetes
- Pod
- Pod networknamespace
- Pause container 
- Kubernetes then invokes the CNI-plugin to join the pause container to a network
- All containers in the pod use the pause network namespace (netns)



Network namespace:
network devices
IP addresses
IP routing tables




Egress & Ingress
----------------



Egress router : Traffic outbound --Going out from Cluster 
-------------

Restricted traffic control 
External resources recognize application/pod traffic 
whitelisted IP addresses 
egress router is not compatible cloud platform that does not support layer 2 manipulations
iptables rules for redirections 
Macvlan : subinterface with additional mac address 
SDN : multitenant/Network-policy 

1. Egress router 

apiVersion: v1
kind: Pod
metadata:
  name: egress-multi
  labels:
    name: egress-multi
  annotations:
    pod.network.openshift.io/assign-macvlan: "true"
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift3/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE 
      value: 192.168.12.99/24
    - name: EGRESS_GATEWAY
      value: 192.168.12.1
    - name: EGRESS_DESTINATION 
      value: |
        80   tcp 203.0.113.25
        8080 tcp 203.0.113.26 80
        8443 tcp 203.0.113.26 443
        203.0.113.27
    - name: EGRESS_ROUTER_MODE
      value: init
  containers:
  - name: egress-router-wait
    image: registry.redhat.io/openshift3/ose-pod
    
    
2. Egress router pod network view 
 
 sh-4.2# ip a
eth0@if26: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP 
inet 10.128.0.81/23 scope global eth0
    
macvlan0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN 
inet 192.168.123.99/32 scope global macvlan0
     

3. Egress service ( Other pods can access via egress router pods)
apiVersion: v1
kind: Service
metadata:
  name: egress-1
spec:
  ports:
  - name: http
    port: 80
  - name: https
    port: 443
  type: ClusterIP
  selector:
    name: egress-1
    
    
- HTTP PROXY
containers:
  - name: egress-router-proxy
    image: registry.redhat.io/openshift3/ose-egress-http-proxy
    env:
    - name: EGRESS_HTTP_PROXY_DESTINATION 
      value: |
        !*.example.com
        !192.168.1.0/24
        *
- DNS PROXY

containers:
  - name: egress-dns-proxy
    image: registry.redhat.io/openshift3/ose-egress-dns-proxy
    env:
    - name: EGRESS_DNS_PROXY_DEBUG 
      value: "1"
    - name: EGRESS_DNS_PROXY_DESTINATION 
      value: |
        # Egress routes for Project "Foo", version 5

        80  203.0.113.25

        100 example.com

        8080 203.0.113.26 80

        8443 foobar.com 443
        
        
 - Enabling failover by replicas: 1
 - EGRESS_DNS_PROXY_DESTINATION using a ConfigMap
 
 4. Egress firewall 
 
 {
    "kind": "EgressNetworkPolicy",
    "apiVersion": "v1",
    "metadata": {
        "name": "default"
    },
    "spec": {
        "egress": [
            {
                "type": "Allow",
                "to": {
                    "cidrSelector": "1.2.3.0/24"
                }
            },
            {
                "type": "Allow",
                "to": {
                    "dnsName": "www.foo.com"
                }
            },
            {
                "type": "Deny",
                "to": {
                    "cidrSelector": "0.0.0.0/0"
                }
            }
        ]
    }
}
       

Egress IPs 
----------
Open for CIDR 

Namespace/Project level : 
- Apply NetNamespace
- Manually - HostSubnet node 
- Dynamically - egressCIDRs Node config file 
- Same subnet as Node 


oc patch netnamespace project1 -p '{"egressIPs": ["192.168.1.100"]}'
oc patch netnamespace project2 -p '{"egressIPs": ["192.168.1.101"]}''
oc patch hostsubnet node1 -p '{"egressCIDRs": ["192.168.1.0/24"]}'
oc patch hostsubnet node2 -p '{"egressCIDRs": ["192.168.1.0/24"]}'


Limiting the Bandwidth Available to Pods
----------------------------------------
QOS Traffic inbound-outbound 

Common :
{
    "kind": "Pod",
    "spec": {
        "containers": [
            {
                "image": "openshift/hello-openshift",
                "name": "hello-openshift"
            }
        ]
    },
    "apiVersion": "v1",
    "metadata": {
        "name": "iperf-slow",
        "annotations": {
            "kubernetes.io/ingress-bandwidth": "10M",
            "kubernetes.io/egress-bandwidth": "10M"
        }
    }
}





CNO (DS)
---
Manage/deploy

CNI 
SDN


INGRESS
-------

1. Router: HTTP/HTTPS/SNI based traffic
2. NodePort
3. IngressController - LB SVC type
   - ingressIPNetworkCIDR
   - Pool ingressIPNetworkCIDR
   - master configuration file (networkConfig: ingressIPNetworkCIDR: 192.168.1.0/24)
   
 Ingress IP for service
------------------------

apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-07-03T18:05:02Z
  labels:
    app: airroute
  name: airroute
  namespace: new
  resourceVersion: "2555900"
  selfLink: /api/v1/namespaces/new/services/airroute
  uid: 1412d90f-9dbd-11e9-bdb7-022f5fa8c0f0
spec:
  clusterIP: 172.30.46.197
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: airroute
    deploymentconfig: airroute
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}


4. Manually assigning External IP to service

   - master configuration file externalIPNetworkCIDR: 172.29.0.0/16   
   
   
   
 for service
----------------------   
{
    "kind": "Service",
    "apiVersion": "v1",
    "metadata": {
        "name": "my-service"
    },
    "spec": {
        "selector": {
            "app": "airroute"
        },
        "ports": [
            {
                "name": "http",
                "protocol": "TCP",
                "port": 8080,
                "targetPort": 8080
            }
        ],
        "externalIPs" : [
            "172.29.10.1"         
        ]
    }
}

    
 
 Manual process IP address assignment and routing
 ------------------------------------------------
 ip address add 172.29.10.1 dev eth0
 route add -net 172.29.10.0/16 gw <Node_IP> eth0
 route add -net 172.29.0.0/16 gw 172.31.17.49 eth0
    
    
* Cloud (Load Balancer services for automatic deployment of a cloud load balancer to target the service’s endpoints) 





Integrating external Services:
------------------------------

Usecase : External Database


Defining SVC
------------

kind: "Service"
  apiVersion: "v1"
  metadata:
    name: "external-mysql-service"
  spec:
    ports:
      -
        name: "mysql"
        protocol: "TCP"
        port: 3306
        targetPort: 3306
        nodePort: 0
  selector: {}
  
Defining EP
-----------

  kind: "Endpoints"
  apiVersion: "v1"
  metadata:
    name: "external-mysql-service" 
  subsets: 
    -
      addresses:
        -
          ip: "10.0.0.0" <---- External DB IP
      ports:
        -
          port: 3306 
          name: "mysql"
          
          
* The port and name definition must match the port and name value in the service defined in the previous step.
  
  
External Name:

Service
-------

kind: "Service"
apiVersion: "v1"
metadata:
  name: "external-mysql-service"
spec:
  type: ExternalName
  externalName: example.domain.name
selector: {} 

Consuming service
---------------

DB server: external-mysql-service.<projectname>.svc.cluster.local
env:
            -
              name: "MYSQL_USER"
              value: "${MYSQL_USER}" 
            -
              name: "MYSQL_PASSWORD"
              value: "${MYSQL_PASSWORD}" 
            -
              name: "MYSQL_DATABASE"
              value: "${MYSQL_DATABASE}" 

env:
              -
                name: "SAAS_API_KEY" 
                value: "<SaaS service API key>"
              -
                name: "SAAS_USERNAME" 
                value: "<SaaS service user>"
              -
                name: "SAAS_PASSPHRASE" 
                value: "<SaaS service passphrase>"



Monitoring & Alert (CMO)
------------------------
Prometheus
Alertmanager 
Grafana 
Node-exporter
kube-state-metrics


Promethues:
Monitoring target configurations based on familiar Kubernetes label queries
time-series data

AlertManager:

Notifications
silences: simply mute alerts for a given time
Inhibition :suppressing notifications 
grouping 
client behaviour 

email
PagerDuty
HipChat

Monitoring Targets:
Promethues itself 
Kubernetes apiserver
kubelets (the 
kube-state-metrics
kubelet embeds cAdvisor for per container metrics)
kube-controllers
node-exporter
etcd 



Sizing Guide:
https://docs.openshift.com/container-platform/3.11/
scaling_performance/scaling_cluster_monitoring.html#cluster-monitoring-recommendations-for-OCP
Storage P: 50 G
Storage A: 2 G

Support: 
https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html#supported-configuration
Alerting Rules
--------------
https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html#alerting-rules

Storage
-------


Factors:
- Type of data that you want to store
- Usage pattern
- Scaling concerns
- Finally your budget


Type of Storage:
- Block Storage ; Raw disks, SAN solutions 
- File Storage; Formatted with a file system like FAT32, NTFS, EXT3, and EXT4, XFS, NAS solutions, Ceph file
- Object Storage; Api/http access 



BlockStorage 
------------
DATA Representation/organization/Hold: Volumes/LUNS 

Use cases:

- Application I/O performance and low-latency connectivity
- Databases
- Application which requires service side processing, like Java, PHP, and .Net will require block storage
- Mission-critical applications like Oracle, SAP, Microsoft Exchange, and Microsoft SharePoint
- Efficency and reliability 

Block storage options on Prem 

- SAN (netapp)
- Ceph RADOS Block Device
- Gluster 
- Cinder
- Dell/EMC Scale.IO
- VMware vSphere Volume

Block storage options in the cloud
- EBS
- Google Persistent Disk
- Azure premium disks 
- Rackspace cloud block storage 

Limitation:
- Block storage volumes can only be accessed when they’re attached to an operating system
- RWO

Protocol : FC, SCSI & iSCSI, FCoE
Media : FC,ETH

File Storage
------------

DATA Representation/organization/Hold: files in folders

- Application scalability
- Simultaneous Read/Write
- RWX
- Cost effective 

Protocol : 
CIFS 
NFS
SMB

Media : Ethernet 


Cloud Options 
--------------

EFS
Azure CFS
Google Cloud Storage 

On Prem:
--------
Ceph
Gluster 
RHEL NFS
NetApp NFS

Object Storage
--------------

DATA Representation/organization/Hold:  objects & Metadata 

- Accessed directly through APIs or http/https
- Object store guarantees that the data will not be lost
- Object storage data can be replicated across different data centers 
- offer simple web services interfaces for access

Use cases:
- User generated data
- Storage of unstructured data like music, image, and video files, Multimedia 
- Storage for backup files, database dumps, and log files
- storage can be used as your big data object store
- Archive files in place of local tape drives

Protocol :
HTTP
API

SAN
NAS
DAS 


Cloud Options 
--------------

Amazon S3
Rackspace Cloud Files
Azure Blob Storage
Google cloud storage

On Prem:
--------
Ceph
Gluster 


https://docs.openshift.com/container-platform/3.11/scaling_performance/optimizing_storage.html


Openshift Persistent storage
----------------------------

Persistent Volume --- Persistent Voume Claims----Volumes

PV-PVC one to one mapping 

Static Provisioning 
Dynamic Provisioning (storage class)

The reclaim policy for a PersistentVolume

Retain:
- PersistentVolume still exists and the volume is considered “released”. 
- Volume not available for another claim because the previous claimant’s data remains on the volume
- An administrator can manually reclaim

Delete
- Delete data and PV

Recycle
- Scrub volume rm -rf 
- volume and makes it available again for a new claim.


Images
-------
Signing
-------
https://docs.openshift.com/container-platform/3.11/admin_guide/image_signatures.html
https://access.redhat.com/articles/2750891#architecture

Scanning
--------



Quay on RH OCP 
--------------

Features of Quay include:

- High availability
- Geo-replication
- Docker v2, schema 2 (multiarch) support
- Continuous integration
- Security scanning
- Custom log rotation
- 24/7 support
- Quay provides support for multiple:
- Authentication and access methods
- Storage backends
- SSL certificates
- Application registries


ARCH
----

3 Core components 

1. DB (Postgres / Mysql) 
   - MetaData storage 
   - Not of image storage 

2. Redis (Key value store)
   - Real time events 
   
3. Quay (container registry) 


HA Quay 
-------

Storage backend 
---------------

- public cloud storage 
  S3, Rackspace Cloud Files, Azure Blob Storage, Google cloud storage
- Privat cloud storage 
  open stack swift, Ceph RADOS, Ceph Object Gateway
  
* Object storage preferred 

HA Data bases 
--------------


Prerequisites

- Cluster-admin OCP or priviledge account for namespace creation 
- HA storage backends
- HA Data bases 


Steps Installation:

oc login -u system:admin
oc create -f quay-enterprise-namespace.yaml 

























MISC
--------------------------------------------------------------------
docker login -u="redhat+quay" -p="O81WSHRSJR14UAZBK54GQHJS0P1V4CLWAJV1X2C4SD7KO59CQ9N3RE12612XU1HR" quay.io

NVMe - Flash, 64 k command que, Latency 2.8 MS, Direct CPU, 1 M IOPS
AHCI - Spin, 1 command que, 6 MS, sata , 100K IOPS 

egion : Geo isolation 
No replication accross regions
Data transfer rates between regions


VPC (Isolated Network,storage,compute)CIDR block
- Span accross AZs


AZ (physical isolated)
Caveat : Storage attachement 



https://github.com/openshift/installer/blob/master/docs/user/metal/install_upi.md
https://blog.openshift.com/deploying-a-user-provisioned-infrastructure-environment-for-openshift-4-1-on-vsphere/

Container Registry
------------------

The registry stores images and metadata. For production environment, persistent storage should be used for the registry, 
otherwise any images that were built or pushed into the registry would disappear if the pod were to restart.
https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb


https://docs.openshift.com/container-platform/3.11/install/host_preparation.html#managing-docker-container-logs
